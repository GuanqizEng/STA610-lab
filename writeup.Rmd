---
title: "Ben Bauchwitz Case Study 1"
output: pdf_document
---


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(tidyverse)
library(tidyr)
library(sjmisc)
library(lme4)
library(brms)
library(lattice)
library(rstan)
library(sjstats)
library(kableExtra)

knitr::opts_chunk$set(echo = F, include = F)
load("data/streetrx.RData")
cities <- read.csv("data/simplemaps_uscities_basicv1.73/uscities.csv")
airports <- read.csv("data/airports.csv")
city.mapping <- read.csv("data/city_mapping.csv")
```

## Introduction  

The goal of this project is to investigate the heterogeneity of price per milligram of morphine by location and its relationship with other factors in a data set collected from `streetrx.com`. Made online since 2010, StreetRx has been collecting citizen-reported street price data of diverted pharmaceutical substances, such as morphine. Since the price of morphine provides information about the demand, availability, and potential drug abuse, the investigation of this project could reveal insights of the morphine price over various locations and other indicators that are helpful in health surveillance for drug abuse and other health-related issues. To this end, we have built a hierarchical model on morphine price with location grouping variable through a the data analysis and modeling process performed in the following orders. First, since the data set entries had inconsistent naming problems in the location information and other issues, data cleaning was conducted to generate a data set ready for modeling. Second, we performed exploratory data analysis to understand the distributions of variables and their potential relationships. Third, we looked into the heterogeneity of the response variable by different scales of locations and chose the proper location variable as the grouping variable with random effect. Then, the other predictors, either factors or numeric variables, were selected to the model by the logical reasoning and model performance once that predictor was added. Possible interactions among predictors, which could be either fixed effects or random slopes, were also investigated in this step. Finally, we built the final model using the results from the previous sections. We checked the validness of the model assumptions and interpreted the model results, and drew the conclusion. **(Add Conclusion Here!)**   

In summary, we performed data cleaning, exploratory data analysis, group variable selection, predictor variable selection and modeling, and finally model checking and interpretation to obtain the conclusion of the project. **( or Add Conclusion Here!)**  

## Data Cleaning 

The original data set contains observations of transactions of various types of drugs. By inspecting the data set values and its structures for each predictor, we found the severla issues with noisy and corrupted data. Here is a summary of the issues and solutions to clean the data set.  

First, in the original data set, `api_temp` variable specifies the active ingredient of the drug for each observation, and we need to filter the entire data set to obtain the morphine transaction observations according to the drug type specified by this variable. However, there were ambiguous labels of the **drug type** in the entries of `api_temp` variable, For example, there were categories specified as `morphine/oxycodone` rather than `morphine` only. Since we did not want observations that were related to other drug types to interfere the morphine price modeling process, we only kept the entries that exclusively listed `morphine`. This filtering process results in a new data set with 9268 records.  

Second, we reformatted the **date data** in the data set. There were two fields representing date information: `yq_pdate` and `price_date`. The first field coded the year and quarter as a pseudo-continuous range. However, it was not suitable for assessing the data on a true continuous scale because it portrayed each quarter as being closer to adjacent quarters in the same year than adjacent quarters in neighboring years (i.e., fourth quarter 2019 was closer to third quarter 2019 than it was to first quarter 2020 in this coding scheme). Therefore, intead of using `yq_pdate`, we used the `price_date` to construct a true continuous scale for measuring the date of purchase. Using R's string parsing methods we parsed the date into its individual *month*, *date*, and *year*, and then creating a new field for each record that counts the *number of elapsed days since an arbitrary reference date*. In this case, we set *January 1, 2010* as the starting date, since the Streetrx data collection began in 2010. In addition, upon inspecting the date data we observed that 3 entries were from the 1960s and an additional 11 entries were from the 2000s, which were years before the data collection was active. In this case, there was legitimate concern that the entered data might not be accurate if it was supplied long after the drug purchase event. Since a relatively small number of data were affected, we filtered out the 14 entries with dates prior to 2010, leaving us with 9254 observations.  

Third, we have assessed data on the city in which the drugs were purchased, and the data set contained **city aliases and ambiguous references to some cities** other than the formal names of cities. We saw that there were 1690 unique "city" values among the 9254 observations. However, when we inspected visually we could see almost immediately that there were numerous entries with **different listed names that clearly refer to the same city**. This was because users might have used different conventions when supplying city names. For example, users might have listed either "Fort Lauderdale" or "Ft Lauderdale" to refer to the same city, resulting in two different values. We saw a range of other common data discrepancies, such as using city nicknames (e.g., "Philly" for Philadelphia), airport codes (e.g., "ATL" for Atlanta), or using the abbreviation used by major sports franchises within the city (e.g., "JAX" for Jacksonville). Various other issues were observed frequently, such as users providing redundant state information (e.g., "Des Moines, IA" instead of just Des Moines), rerring to a city by the specific neighborhood or borrough (e.g., "Brooklyn" instead of "New York") or including single character typos (e.g., "Holywood" instead "Hollywood"). We employed the following two approaches to address this issue.  

In most cases, the original city could be unambiguously identified and corrected manually. First, we imported US census data which defined the official city abbreviation used by each city and cross referenced it with the listed data. Fortunately, this process covered the majority of the entries, but there were still about 300 city values supplied that were not on the list. Therefore, we manually created a new dictionary mapping the noisy values to the corrected values, in the cases identified above where the data could be definitively determined. Then, we applied this mapping to the original data to correct those entries which were non-compliant. During this process we also identified legitimate names of unincorporated areas and townships which were excluded from the original census data, so that we would not be unnecessarily throwing out data from real cities. 

In other cases, the original city could not be unambiguously identified from the data given. For example, some users listed their zip code, which often crossed city lines. Others listed their county, which included multiple cities, and others listed the general metropolitan areas (e.g., "Lehigh Valley" or "Dallas - Fort Worth"). We did not want to guess, so in these cases, we replaced any remaining city names with **Other/Unknown**.

One pitfall that we had to avoid was inappropriately aggregating city data that were not related. For example, the data set contained both "Hollywood, FL" and "Hollywood, CA". If we ultimately built a hierarchical model with both state and city grouping variables, we did not want to mistakenly label data from those two places as being from different states but the same city. Therefore, we augmented our coding of the city name by appending the state as well so that each city was uniquely encoded, even if it shared its name with another city in a different state.  

Then, we inspected the source data. Most entries were labeled with "Personal" or "Heard it" as the source, but there were still over 50 unique entries, too many to do serious grouping on. However, when we inspected each unique entry, we saw some common themes. First, many users entered the specific URL for various web pages they searched, and several web pages were represented repeatedly with different URLs. But more importantly, there were clear patterns in the types of sources listed. In particular, we observed that each of the sources was one of the following: (A) personal, (B) word of mouth, (C) a web forum, (D) an online black market, (E) a legal online market, (F) a web search, or (G) other/unknown. In fact, we could very easily convert the raw data to these categories by using sub-string search to find particular keywords, such as "silkroad", "bluelight", "reddit", "opiophile", "forum", "pharmacy" and various search engine names. As a result, we bundled each source into one of those seven categories, making for much simpler and more informative grouping. This data cleaning step did not change the size of the data set.  


Our final data cleaning step included many minor issue corrections and basic cleaning of other predictor. First, we noticed that the state field included both the `state` values "USA" and "". We converted these values to "Other/Unknown" category since they did not provide any meaningful information. Second, we saw that `bulk_purchase` was coded as a string, which wasn't helpful, so we converted it to a numeric boolean (0 = False, 1 = True). Third, we observed that there was nothing listed under the `Primay_Reason` field when the user did not enter the reason for the purchase. We marked such entries with missing values as "Other/Unknown" as we have with the other fields.  

 


```{r , warning=FALSE, error=FALSE}
## Step 1: filter out data on other drugs so we only have the drug we are studying 
## drug type 
streetrx.m <- streetrx %>% filter(api_temp == "morphine")
```


```{r , warning=FALSE, echo=FALSE}
## Step 2: parse the date and filter out years before 2010
date_parts <- str_split(streetrx.m$price_date, '/', 3)
date_parts_df <- as.data.frame(do.call(rbind, date_parts))
streetrx.m$Date_Month <- date_parts_df$V1
streetrx.m$Date_Day <- date_parts_df$V2
streetrx.m$Date_Year <- date_parts_df$V3
streetrx.m$Days_since_010110 <- as.Date(as.character(streetrx.m$price_date), format="%m/%d/%Y")-as.Date(as.character("1/1/10"), format="%m/%d/%Y")
streetrx.m <- streetrx.m %>% filter(Date_Year < 22)
streetrx.m <- streetrx.m %>% filter(Date_Year >= 10)
```


```{r , warning=FALSE, error=FALSE}
## Step 3: fix noisy city data, including typos, nicknames, and alternate identifications by cross-referencing official city names and common alternate spellings
## fill in the missing values
streetrx.m$city <- as.character(streetrx.m$city)
streetrx.m[streetrx.m$city == "",]$city <- "Other/Unknown"

## convert names to upper case to enable matching across cases used
streetrx.m$city <- sapply(streetrx.m$city, toupper)
cities$city <- sapply(cities$city, toupper)
city.mapping$Old_name <- sapply(city.mapping$Old_name, toupper)
city.mapping$New_name <- sapply(city.mapping$New_name, toupper)

## find case-corrected city names that match the official database
city.intsct <- intersect(streetrx.m$city, cities$city)
city.intsct.df <- as.data.frame(do.call(rbind, as.list(city.intsct)))
city.intsct.df$Updated_city_name <- city.intsct.df$V1
streetrx.m <- merge(x = streetrx.m, y = city.intsct.df, by.x = "city", by.y = "V1", all.x = TRUE)

## For cities that are not in the official city database, check if they are in the dictionary of misprints
colnames(city.mapping)[2] = "Updated_city_name"
streetrx.m <- merge(x = streetrx.m, y = city.mapping, by.x = "city", by.y = "Old_name", all.x = TRUE)
streetrx.m <- streetrx.m %>% mutate(Updated_city_name.x = coalesce(Updated_city_name.x, Updated_city_name.y))

## If the name still wasn't found, fill in with "Other/Unknown"
streetrx.m$Updated_city_name.x <- as.character(streetrx.m$Updated_city_name.x)
streetrx.m[is.na(streetrx.m$Updated_city_name.x),]$Updated_city_name.x <- "Other/Unknown"
streetrx.m$City_final = paste(streetrx.m$Updated_city_name.x, "-", streetrx.m$state)
streetrx.m$City_final <- as.factor(streetrx.m$City_final)
```


### Aggregating source data

```{r , include=FALSE, warning=FALSE, echo=FALSE, error=FALSE}
## Step 4: convert the source data to a factor with 7 levels:
## (1) word of mouth   (2) online search   (3) online forum   (4) personal
## (5) online black market    (6) other online market   (7) other/unknown
streetrx.m$source <- as.character(streetrx.m$source)
streetrx.m$Source_class <- "Other/Unknown"
streetrx.m[grep("silkroad", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Online black market"
streetrx.m[grep("bluelight", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Online forum"
streetrx.m[grep("reddit", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Online forum"
streetrx.m[grep("opiophile", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Online forum"
streetrx.m[grep("forum", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Oneline forum"
streetrx.m[grep("Pharmacy", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Other online market"
streetrx.m[streetrx.m$source == "Heard it",]$Source_class = "Word of mouth"
streetrx.m[streetrx.m$source == "Personal",]$Source_class = "Personal"
streetrx.m[streetrx.m$source == "Internet",]$Source_class = "Online search"
streetrx.m[grep("google", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Online search"
streetrx.m[grep("yahoo", streetrx.m$source, ignore.case = TRUE),]$Source_class = "Online search"
streetrx.m$Source_class <- as.factor(streetrx.m$Source_class)
```



### Cleaning other fields

```{r , include=FALSE, warning=FALSE, echo=FALSE, error=FALSE}
## Step 5: eliminate noisy state data by converting the states "USA" and "" to "Other/Unknown"
streetrx.m$state <- as.character(streetrx.m$state)
streetrx.m[streetrx.m$state == "",]$state <- "Other/Unknown"
streetrx.m[streetrx.m$state == "USA",]$state <- "Other/Unknown"
streetrx.m$state <- as.factor(streetrx.m$state)

## Step 6: convert the bulk purchase data to a numeric boolean (0 = false, 1 = true)
bulk_parts <- str_split(streetrx.m$bulk_purchase, ' ', 2)
bulk_parts_df <- as.data.frame(do.call(rbind, bulk_parts))
streetrx.m$Bulk <- as.numeric(bulk_parts_df$V1)

## Step 7: clean up the 'primary reason' variable by marking missing data as "Other/Unknown"
streetrx.m$Primary_Reason <- as.character(streetrx.m$Primary_Reason)
streetrx.m[streetrx.m$Primary_Reason == "",]$Primary_Reason <- "13 Not reported"
streetrx.m$Primary_Reason <- as.factor(streetrx.m$Primary_Reason)

## Step 8: filter out the temporary columns to keep only the processed data
streetrx.m <- subset(streetrx.m, select=-c(city, yq_pdate, price_date, country, source, api_temp, bulk_purchase, Updated_city_name.x, Updated_city_name.y))
streetrx.m.comp <- streetrx.m[complete.cases(streetrx.m),]

## Step 9: filter out the observations where ppm equals to zero 
## price cannot be 0; online market
streetrx.m.comp[which(streetrx.m.comp$ppm == 0),] 
streetrx.m.comp.pppm <- streetrx.m.comp %>% filter(ppm != 0)
```



```{r rename data}
data <- streetrx.m.comp.pppm
```


## Exploratory Data Analysis

In this part, we will explore the distributions of numeric data, potential relationships we might include in the model, and potential correlations.

### Examining distributions of the numeric data


```{r}
summary(data)
```

**Response Variable**

```{r}
ggplot(data) +
  geom_density(aes(x = ppm)) +
  theme_bw(base_size=16)
```

Obviously, the response variable is highly right skewed, so we try to log transform it. 

```{r}
# response transform
ggplot(data) + 
  geom_density(aes(x = log(ppm))) +
  theme_bw(base_size = 16)
```

**Group Variables** 

1. City 


```{r}
set.seed(100)
sample_city <- sample(unique(data$City_final), 25, replace=FALSE)
ggplot(data[is.element(data$City_final, sample_city),], 
       aes(x=City_final, y=log(ppm), fill=City_final)) + 
  geom_boxplot() + 
  labs(title="log ppm by city", 
       y="log(ppm)") + theme_classic() + 
  theme(legend.position="none", 
        axis.text.x= element_text(angle=90, hjust=1),
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5))
```

2. State 

```{r}
set.seed(100)
sample_state <- sample(unique(data$state), 25, replace=FALSE)
ggplot(data[is.element(data$state, sample_state),], 
       aes(x=state, y=log(ppm), fill=state)) + 
  geom_boxplot() + 
  labs(title="log ppm by state", 
       y="log(ppm)") + theme_classic() + 
  theme(legend.position="none", 
        axis.text.x= element_text(angle=90, hjust=1),
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5))
```

3. Region 

```{r}
ggplot(data, aes(x=USA_region, y=log(ppm), fill=USA_region)) + 
  geom_boxplot() + 
  labs(title="log ppm by USA_region", 
       y="log(ppm)") + theme_classic() + 
  theme(legend.position="none", 
        axis.text.x= element_text(angle=90, hjust=1),
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5))
```

In summary, from the above boxplots, it seems that `city_Final` is a very good grouping variable, though some cities have quite small sample sizes, which is not good for a grouping variable. `state` might be a good grouping variable, but some of the `state` do not vary in the response variable `log(ppm)`, which is not good for a grouping variable. It seems that `USA_region` is not a good grouping variable since the `log(ppm)` seems not to vary across `USA_region`.  


**Fixed Effect Predictors** 

1. Days 

First, consider fixed effects for days, which are not by groups. 
```{r}
plot(log(data$ppm)~data$Days_since_010110) #does not seem to have relationship
```

Second, consider the effects of days in each group, here, presumably, `state`. 

```{r}
set.seed(108)
sample_state <- sample(unique(data$state), 6, replace=FALSE)
ggplot(data[is.element(data$state, sample_state), ], aes(x=Days_since_010110, y=log(ppm), color=state)) +
  geom_point() + 
  geom_smooth(method="lm", fill=NA)
```
It seems that in most states, the price does not change according to `days`. It might worthing trying to add fixed affects of `days` by `state` to the model.  


2. Primary Reason 

```{r}
ggplot(data, aes(x=Primary_Reason, y=log(ppm), fill=Primary_Reason)) +
  geom_boxplot() + 
  labs(title="log ppm by Primary Reason", 
       y="log(ppm)") + 
  theme_classic() + 
  scale_x_discrete(labels=c("0 No Answer", "10 Pain", "11 Come Down", "13 No Report", "3 Deal w/ Withdrawal", "4 Get High", "5 Resell", "6 Other", "7 DK", "8 Private", "9 Self-treat Pain")) + 
  theme(legend.position="none", 
        axis.text.x= element_text(angle=90, hjust=1),
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5)) 
```

What is the difference between the group 0 and group 13? 

It seems that, in general, `log(ppm)` does not differ too much across different `Primary_Reason` values. 

```{r}
set.seed(102)
sample_state <- sample(unique(data$state), 4, replace=FALSE)
ggplot(data[is.element(data$state, sample_state), ], aes(x=Primary_Reason, y=log(ppm), fill=Primary_Reason)) +
  geom_boxplot() + 
  labs(title="log ppm vs Primary Reason by state", 
       y="log(ppm)") + theme_classic() + 
  scale_x_discrete(labels=c("0", "10", "11", "13", "3", "4", "5", "6", "7", "8", "9")) + 
  theme(legend.position="none",
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  facet_wrap(~state, ncol=2)
```

```{r}
set.seed(103)
sample_state <- sample(unique(data$state), 4, replace=FALSE)
ggplot(data[is.element(data$state, sample_state), ], aes(x=Primary_Reason, y=log(ppm), fill=Primary_Reason)) +
  geom_boxplot() + 
  labs(title="log ppm vs Primary Reason by state", 
       y="log(ppm)") + theme_classic() + 
  scale_x_discrete(labels=c("0", "10", "11", "13", "3", "4", "5", "6", "7", "8", "9")) + 
  theme(legend.position="none",
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5)) + 
  facet_wrap(~state, ncol=2)
```

It seems that, with-in each `state`, the `log(ppm)` differ according to different `Primary_Reason` values. Therefore, it is possible to include random effects of `Primary_Reason` by `state` in the model.  


3. Source 

```{r}
ggplot(data, aes(x=Source_class, y=log(ppm), fill=Source_class)) +
  geom_boxplot() + 
  labs(title="log ppm by Source_class", 
       y="log(ppm)") + 
  theme_classic()  + 
  theme(legend.position="none", 
        axis.text.x= element_text(angle=90, hjust=1),
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5)) 
```

It seems that in general, the `log(ppm)` varies according to the value of the `Source_class` predictor.  

```{r}
set.seed(103)
sample_state <- sample(unique(data$state), 8, replace=FALSE)
ggplot(data[is.element(data$state, sample_state), ], aes(x=Source_class, y=log(ppm), fill=Source_class)) +
  geom_boxplot() + 
  labs(title="log ppm vs Source by state", 
       y="log(ppm)") + theme_classic() + 
  theme(legend.position="none",
        axis.title.x = element_blank(), 
        axis.text.x= element_text(angle=90, hjust=1), 
        plot.title = element_text(hjust=0.5)) + 
  facet_wrap(~state, ncol=4)
```

I think it might be worth trying to add `Source_class` as a random slope by `state` to the model.  


4.Bulk  

```{r}
ggplot(data, aes(x=as.factor(Bulk), y=log(ppm), fill=as.factor(Bulk))) +
  geom_boxplot() + 
  labs(title="log ppm by Bulk", 
       y="log(ppm)") + 
  theme_classic()  + 
  theme(legend.position="none", 
        axis.text.x= element_text(angle=90, hjust=1),
        axis.title.x = element_blank(), 
        plot.title = element_text(hjust=0.5)) 
```

```{r}
set.seed(104)
sample_state <- sample(unique(data$state), 8, replace=FALSE)
ggplot(data[is.element(data$state, sample_state), ], aes(x=as.factor(Bulk), y=log(ppm), fill=as.factor(Bulk))) +
  geom_boxplot() + 
  labs(title="log ppm vs Bulk by state", 
       y="log(ppm)") + theme_classic() + 
  theme(legend.position="none",
        axis.title.x = element_blank(), 
        axis.text.x= element_text(angle=90, hjust=1), 
        plot.title = element_text(hjust=0.5)) + 
  facet_wrap(~state, ncol=4)
```

5. Dosage Strength 

```{r ,warning=FALSE, error=FALSE}
##test_norm(streetrx.m.comp$ppm)
##test_norm(log(streetrx.m.comp$ppm + 0.01))
levels(as.factor(data$mgstr))
```

```{r}
plot(log(data$ppm)~data$mgstr) # does not seem to have relationship; slightly negative
```


```{r}
set.seed(105)
sample_state <- sample(unique(data$state), 8, replace=FALSE)
ggplot(data[is.element(data$state, sample_state), ], aes(x=as.factor(mgstr), y=log(ppm), fill=as.factor(mgstr))) +
  geom_boxplot() + 
  labs(title="log ppm vs Dosage Strength by state", 
       y="log(ppm)") + theme_classic() + 
  theme(legend.position="none",
        axis.title.x = element_blank(), 
        axis.text.x= element_text(angle=90, hjust=1), 
        plot.title = element_text(hjust=0.5)) + 
  facet_wrap(~state, ncol=4)
```

According to the above results, the result indicates that it is preferred to include the random slopes of `dosage strength` by `state` in the model. 


Our data processing from above results in three numeric variables and several categorical variables. First, we examine the `ppm` variable, our primary outcome variable. Even though the values don't vary over many orders of magnitude, it has extreme right skew. Taking the log of these values appears to dramatically improve various indicators of normality, includng the Q-Q plot and histogram, and while the data still fails a Shapiro-Wilk normality assessment this transform appears reasonable for the data. 

A second issue we observe is in the reported purchase quantities (the variable `mgstr`). Although this variable is numeric, values are reported at discrete intervals, perhaps due to standard packaging sizes or users rounding off the values that they reported. All told, there are 16 unique values ranging from 1 to 200. We have two options: treat this as a categorical variable or accept the sparse numeric coding. If we do ultimately evaluate `mgstr` in our model, it does not seem that we would be interested in categorical relationships. In other words, we don't have any reason to believe there is something unique about individual package size levels. Instead, we are concerned with the general trend in how different volumes affect price. Since 16 points is more than adequate to fit a line in most applications and since the data we've collected has multiple volume sizes at each order of magnitude, it seems reasonable to continue with the numeric encoding.

The final numeric variable we consider is the linear temporal variable we created, which is number of days since Jan 1, 2010. This lets us encode time in a semi-continuous manner. Despite the discreet encoding, the data actually functions quite well as a numeric predictor since there is high resolution relative to the overall time scale. The only thing that is apparent is that when we plot the price data against over time, the data is clearly sparser earlier in streetrx's history. However, with the appropriate parameter selection, this should not be a significant issue in the overall models we will create.


### Examining Relationships

```{r}
ggplot(data = data,
       aes(x = Primary_Reason, y = log(ppm), color = Primary_Reason)) +
  geom_violin() +
  geom_jitter(position = position_dodge(0.5), alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))+
  theme(legend.position = "none")+
  xlab("Purchase Rationale") +
  ylab("log(ppm)") +
  ggtitle(label="Relationship between Purchase Rationale and log(ppm)")
```

```{r}
ggplot(data = data,
       aes(x = Source_class, y = log(ppm), color = Source_class)) +
  geom_violin() +
  geom_jitter(position = position_dodge(0.5), alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))+
  theme(legend.position = "none")+
  xlab("Information Source") +
  ylab("log(ppm)") +
  ggtitle(label="Relationship between Information Source and log(ppm)")
```

```{r}
boxplot(log(data$ppm)~data$Bulk)
table(data$mgstr,data$Bulk)
```

## Model design

Our processed dataset includes 12 possible variables we can use to predict drug purchase price: location data including region, state, and city; temporal data including month, date, year, and days elapsed since a reference point; purchase format data including the quantity, form, and whether the purchase was in bulk; and other information including the purchase rationale and the source of the data. 

One thing we note is that several of these variables are redundant. For example, if geographic variation captures significant variability in the purchase price, then using state as a predictor alone may be more valuable than combining state with region, which is somewhat arbitrarily segmented. Similarly, the date and year in which drugs were purchased will have limited additional value beyond the linear time encoding we already have. Finally, the subjective bulk purchase indicator variable is not informative given the objective purchase quantity data we already have, so we most likely would not want to include this term in the model.

I propose a model that accounts for the following predictors:
- Location: a hierarchy of random effects for the state and city in which the drugs were purchased
- Time: encoded as a linear covariate, plus month as a categorical indicator of seasonal trends (since drug supply may vary in different months due to aggricultural considerations)
- Purchase size as a linear covariate
- Purchase rationale as a categorical predictor
- Information source as a categorical predictor

We will also want to consider interaction effects. Different states and cities may have different baseline prices, but they also may have different market trends. For example, the effect of increased purchase size on price may vary across locations depending on how risky it is to carry larger quantities of drugs in different jurisdictions. Similarly, temporal price trends may vary depending on new local regulations that are enacted in some places but not others. Therefore, the model should include not just random intercepts for state and city, but also random slopes for both purchase date and purchase size. 

### Group Variable Checking 

```{r}
group_model_city <- lmer(log(ppm) ~ 1 + (1|City_final), REML=FALSE, data)
summary(group_model_city)
```
```{r results=FALSE}
# coef(group_model_city)
```

```{r}
group_model_state <- lmer(log(ppm) ~ 1 + (1|state), REML=FALSE, data)
summary(group_model_state)
```

```{r message=FALSE, warning=FALSE}
sjPlot::plot_model(group_model_state, type="re")
```

```{r}
group_model_region<- lmer(log(ppm) ~ 1 + (1|USA_region), REML=FALSE, data)
summary(group_model_region)
```
```{r}
sjPlot::plot_model(group_model_region, type="re")
```



```{r}
group_model_city_state <- lmer(log(ppm) ~ 1 + (1|state) + (1|City_final), REML=FALSE, data)
group_model_city_region <- lmer(log(ppm) ~ 1 + (1|City_final) + (1|USA_region), REML=FALSE, data)
group_model_state_region <- lmer(log(ppm) ~ 1 + (1|state) + (1|USA_region), REML=FALSE, data)
group_model_city_state_region <- lmer(log(ppm) ~ 1 + (1|state) + (1|City_final) + (1|USA_region), REML=FALSE, data)
```



**Compare Group Models** 


```{r}
# group_model_city_state_region wins 
anova(group_model_city_region, group_model_city_state_region)
```

```{r}
# group_model_state_region wins 
anova(group_model_state_region, group_model_city_state_region)
```


```{r}
# group_model_city_state wins 
anova(group_model_city_state, group_model_city_state_region)
```
From the above result, we could see that, according to the bic score, `group_model_city_state`, `group_model_state_region` < `group_model_city_state_region` < `group_model_city_region`. 

Then, we apply the following comparisons. 

```{r}
# group_model_city_state wins 
anova(group_model_city, group_model_city_state)
```

```{r}
# group_model_state_wins 
anova(group_model_city_state, group_model_state)
```

```{r}
# group_model_state wins 
anova(group_model_state_region, group_model_state)
```

```{r}
# group_model_state_region wins 
anova(group_model_state_region, group_model_region)
```


Now, we have built the hierarchy as follows, according to the bic score, ranking according to the value of the bic score. The smaller bic score, the better.  

`group_model_city_state`, `group_model_state_region` < `group_model_city_state_region` < `group_model_city_region`. 

`group_model_state` < `group_model_city_state` < `group_model_city`
`group_model_state` < `group_model_region_state` < `group_model_region`. 


```{r}
base <- lmer(log(ppm) ~ (1|state), REML=FALSE, data)
```


**Add dosage strength to the model** 
At this point, I use `state` as the random effect grouping variable.  

```{r}
fixed_mgstr_model <- lmer(log(ppm) ~ (1|state) + mgstr, REML=FALSE, data)
# random_mgstr_model <- lmer(log(ppm) ~ as.factor(mgstr) + (as.factor(mgstr)|state), data)
# mgstr as random effects take forever to run 

anova(base, fixed_mgstr_model)
```
```{r}
fixed_mgstr_model <- lmer(log(ppm) ~ (1|state) + mgstr, REML=FALSE, data)
random_mgstr_model <- lmer(log(ppm) ~ mgstr + (mgstr|state), data)
# mgstr as random effects take forever to run 

anova(random_mgstr_model, fixed_mgstr_model)
```
It seems that `mgstr` with random slope has lower bic score, but the modeling fails to converge, which is a little bit worrying. 

**Adding Bulk to the Model** 

```{r}
add_bulk_model <- lmer(log(ppm) ~ (1|state) + Bulk + mgstr, REML=FALSE, data)
add_bulk_interaction_model <- lmer(log(ppm) ~ (1|state) + Bulk*mgstr, REML=FALSE, data)

anova(add_bulk_model, add_bulk_interaction_model)
anova(add_bulk_model, fixed_mgstr_model)
```

```{r}
add_bulk_model <- lmer(log(ppm) ~ (mgstr|state) + Bulk + mgstr, REML=FALSE, data)
add_bulk_interaction_model <- lmer(log(ppm) ~ (mgstr|state) + Bulk*mgstr, REML=FALSE, data)

anova(add_bulk_model, add_bulk_interaction_model)
anova(add_bulk_model, random_mgstr_model)
```
From the above results, it seems that adding `Bulk` will decrease the bic value, though it does not decrease the bic value so much. Therefore, I decide to add `Bulk` to the model.  


**Adding Primary Reason**  
Now, the base model becomes the following. 

```{r}
base <- lmer(log(ppm) ~ (1|state) +  mgstr, REML=FALSE, data) 
```

```{r}
add_pr_base_model <- lmer(log(ppm) ~ (1|state) + Primary_Reason+ mgstr, REML=FALSE, data) 
anova(add_pr_base_model, fixed_mgstr_model)
```


```{r}
add_pr_base_model <- lmer(log(ppm) ~ (mgstr|state) + Primary_Reason+ mgstr, REML=FALSE, data) 
anova(add_pr_base_model, random_mgstr_model)
```
The `Primary Reason` does not decreases the bic score a lot.

If adding `Primary Reason` to bulk, then, the `Primary Reason` seems to be uselesss.  

```{r}
add_pr_model <- lmer(log(ppm) ~ (1|state) + Primary_Reason+ Bulk + mgstr, data)

anova(add_pr_model, add_bulk_model)
```
From the result, we could see that `Primary_Reason` does not descrease the bic value, so it is not included in the model.  

```{r}
add_random_pr_model <- lmer(log(ppm) ~ (Primary_Reason|state) + Primary_Reason+ Bulk + mgstr, data)

anova(add_random_pr_model, add_bulk_model)
```
Adding `Primary Reason` as a random effect to the model significantly increases the bic value. 

**Adding Source** 

```{r}
base <- lmer(log(ppm) ~ (1|state) +  mgstr, REML=FALSE, data) 
```

```{r}
add_source_model <- lmer(log(ppm) ~ (1|state) + Source_class + mgstr, REML=FALSE, data) 
anova(add_source_base_model, base)
```
```{r}
add_source_pr_model <- lmer(log(ppm) ~ (1|state) + Primary_Reason + Source_class + mgstr, REML=FALSE, data) 
anova(add_source_pr_model, add_source_model)
```
Note that, `Source_class` is preferred to be included in the model than `Primary_Reason`.  

```{r}
add_source_bulk_model <- lmer(log(ppm) ~ (1|state) + Source_class + Bulk + mgstr, data)
anova(add_source_bulk_model, add_bulk_model)
```
Adding `Source` to the model does not decrease the bic value of the model. 

```{r}
# interaction between source and bulk 
add_source_bulk_interaction_model <- lmer(log(ppm) ~ (1|state) + Source_class * Bulk + mgstr, data)
anova(add_source_bulk_interaction_model, add_bulk_model)
```
The interaction between `Source_class` and `Bulk` does not work, either.  

```{r}
# Adding source as a random slope to the model  
add_random_source_model <- lmer(log(ppm) ~ (Source_class|state) + Source_class +  Bulk + mgstr, data)
anova(add_random_source_model, add_bulk_model)

```
The `Source_class` as a random effect significantly increases the bic score of the model. 

**Adding Days** 
Now, the base model becomes the following. 

```{r}
base <- lmer(log(ppm) ~ (1|state) + Source_class + mgstr, REML=FALSE, data)
```


```{r}
add_days_model <- lmer(log(ppm) ~ (1|state) + Days_since_010110 +  Source_class + mgstr, REML=FALSE, data)

anova(add_days_model, base)
```
They seems to be of equivallent bic score.  


```{r}
# add days with random slope 
add_random_days_model <- lmer(log(ppm) ~ (Days_since_010110|state) + Days_since_010110 +  Source_class + mgstr, REML=FALSE, data)

anova(add_random_days_model, add_days_model)
```
No significant different between this model and the original one. So `Days_since_010110` is not added to the model.  


Finally, one model comes out of the comparisons among, `mgstr`, `Bulk`, `Primary_Reason`, `Source_class`, and `Days`, is the following. 
```{r}
model <- lmer(log(ppm) ~ (1|state) + Days_since_010110 + Bulk + mgstr, REML=FALSE, data)
model2 <- lmer(log(ppm) ~ (1|state) + Bulk +  mgstr, REML=FALSE, data) 
model3 <- lmer(log(ppm) ~ (Days_since_010110|state) + Days_since_010110 + Bulk + mgstr, REML=FALSE, data)
anova(model3, model2)
```
```{r}
model3 <- lmer(log(ppm) ~ (Days_since_010110|state) + Days_since_010110 + Bulk + mgstr, REML=FALSE, data)
model4 <- lmer(log(ppm) ~ (Days_since_010110|state) + Days_since_010110 + Bulk + mgstr, REML=FALSE, data)
```
```{r}
icc(model)
```

```{r}
icc(model2)
```




```{r}
summary(model3)
```
```{r}
summary(model)
```





We also have to investigate whether we have to include the random slopes of `mgstr` to the model. 

```{r }
model1 <-brm(log(ppm) ~ (1|City_final) + (1|state)  + mgstr + Primary_Reason + Source_class + Days_since_010110, 
             data = streetrx.m.comp.pppm,
             iter = 7000,
             warmup = 2000,
             family = gaussian())
```

```{r}
summary(model1)
```

```{r}
model2 <- brm(log(ppm) ~ (1|City_final)*Source_class + (1|state) + mgstr + Primary_Reason + Days_since_010110, 
                warmup = 2000,
                data = streetrx.m.comp.pppm, 
                family = gaussian(), 
                iter = 7000)
```

```{r}
summary(model2)
```


```{r}
#seasonal effects
model3 <- blmer(log(ppm) ~ (1|City_final) + (1|state) + mgstr + Primary_Reason + Source_class + Date_Month + Days_since_010110, 
                data = streetrx.m.comp.pppm, 
                warmup = 2000,
                family = gaussian(), 
                iter = 7000)
summary(model3)
```

## Results

## Potential Limitations